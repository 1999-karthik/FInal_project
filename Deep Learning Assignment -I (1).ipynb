{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b42caa",
   "metadata": {},
   "source": [
    "\n",
    "# APPROACH:-\n",
    "\n",  
    "## 1. Neural Networks:\n",
    "A neural network is a system of algorithms that attempts to recognize underlying relationships in a set of data. It emulates the way the human brain works.\n",
    "\n",
    "## 2. Architecture:\n",
    "In the problem, we were dealing with a fully connected feedforward neural network with:\n",
    "- **Input layer**: Initial data for the neural network.\n",
    "- **Hidden layers**: Intermediate layer between input and output.\n",
    "- **Output layer**: Final decision-making layer.\n",
    "\n",
    "## 3. Weight Initialization (`init_weights`):\n",
    "In a neural network, weights are the adjustable parameters that the network learns. Properly initializing these weights can greatly affect the training performance.\n",
    "- **Random Initialization**: We initialized weights randomly from a normal distribution.\n",
    "\n",
    "## 4. Activation Functions:\n",
    "Activation functions introduce non-linearity to the model, allowing the network to learn from error and make adjustments, which is essential for learning complex patterns.\n",
    "- **Sigmoid Function**: It squashes values between 0 and 1. It is commonly used for binary classification.\n",
    "\n",
    "## 5. Feedforward (`feedforward`):\n",
    "This is the process neural networks use to turn the input into an output.\n",
    "- We took the input data and passed it through the network, layer by layer, until the output layer was reached.\n",
    "- The data is multiplied by the weights and biased, and then passed through an activation function.\n",
    "\n",
    "## 6. Loss Function (`loss`):\n",
    "This measures how far off our predictions are from the actual values.\n",
    "- **Mean Squared Error (MSE)**: It measures the average squared differences between predicted and actual values. It's widely used for regression problems but can also be used in classification.\n",
    "\n",
    "## 7. Backpropagation (`backprop`):\n",
    "It is the method used to update the neural network's weights.\n",
    "- We computed the gradient of the loss function with respect to each weight by the chain rule.\n",
    "- We then updated the weights in the direction that reduces the loss.\n",
    "\n",
    "## 8. Training (`train`):\n",
    "Training a neural network means adjusting its weights based on the data and the defined loss function.\n",
    "- We repeatedly applied the feedforward and backpropagation operations over a defined number of iterations (epochs).\n",
    "- After each epoch, we updated the weights to minimize the error.\n",
    "\n",
    "## 9. Predictions (`predict`):\n",
    "Once the model has been trained, it can make predictions. We fed a new input into the trained network and obtained the output.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8345b",
   "metadata": {},
   "source": [
    "# LET US FIRST INITIALISE THE WEIGHTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f5ff0a",
   "metadata": {},
   "source": [
    "## LET US START WITH FIRST FUNCTION\n",
    "\n",
    "## `init_weights` Function Explanation\n",
    "\n",
    "This function initializes the weights for a fully connected neural network.\n",
    "\n",
    "### Parameters:\n",
    "- **`n_inputs`**: This indicates the number of input nodes.\n",
    "- **`n_hidden`**: This represents the number of nodes in each of the two hidden layers.\n",
    "- **`n_output`**: This is the number of output nodes.\n",
    "\n",
    "### Inside the function:\n",
    "\n",
    "1. **Weight Matrix `W0`**:\n",
    "    - Connects the input layer to the first hidden layer.\n",
    "    - Its dimensions are `(n_inputs + 1, n_hidden)`. We add 1 to account for the bias node.\n",
    "    - Initialized randomly using a normal distribution.\n",
    "\n",
    "2. **Weight Matrix `W1`**:\n",
    "    - Connects the first hidden layer to the second hidden layer.\n",
    "    - Dimensions are `(n_hidden + 1, n_hidden)`, with the additional 1 for the bias node.\n",
    "    - Initialized randomly using a normal distribution.\n",
    "\n",
    "3. **Weight Matrix `W2`**:\n",
    "    - Connects the second hidden layer to the output layer.\n",
    "    - Dimensions are `(n_hidden + 1, n_output)`.\n",
    "    - Initialized randomly using a normal distribution.\n",
    "\n",
    "### Returns:\n",
    "The function returns the three weight matrices: `W0`, `W1`, and `W2`.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "After defining our function, we then test it. We initialize a neural network with:\n",
    "- 10 nodes in the input layer.\n",
    "- 5 nodes in each of the two hidden layers.\n",
    "- 3 nodes in the output layer.\n",
    "\n",
    "We then print out the initialized weights for `W0`, `W1`, and `W2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07fc86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.29993947,  0.13788758, -1.07631303, -0.68016606, -1.15312932],\n",
       "        [ 0.09221565,  0.61257967, -0.97300133,  1.12316341,  0.53495125],\n",
       "        [ 0.22216226,  0.0224775 ,  1.66905806, -0.64047523, -1.04593303],\n",
       "        [-0.29761435, -0.45045373,  0.04203176, -0.50638727, -1.25595819],\n",
       "        [ 0.42881808, -0.37531254,  1.1830806 , -1.19682885, -0.95179684],\n",
       "        [ 1.87365521,  1.15504088,  0.45363739,  1.06597301,  0.15452554],\n",
       "        [-0.55625774, -0.98304081, -0.64691355, -0.52090097,  0.68190889],\n",
       "        [-1.68681552, -0.04825258,  1.27320166, -0.65022106, -1.24150116],\n",
       "        [ 1.17079087,  0.02805608,  2.26899707, -0.63604714,  0.23275924],\n",
       "        [ 0.18311584,  0.1127135 , -0.67584401,  0.02156453,  0.51861442],\n",
       "        [ 0.6116853 , -1.26759622,  0.66129634, -1.50376682,  0.39514773]]),\n",
       " array([[-1.20099435,  2.14924263,  1.13635234,  1.38092987,  0.9582902 ],\n",
       "        [ 0.3914314 , -0.14233191, -0.74220784,  3.43380691, -0.03390262],\n",
       "        [-0.96602902,  0.37228626,  0.20366356,  0.81935021, -0.80536988],\n",
       "        [-1.22301526, -0.74390761,  0.00995839,  0.41458287, -0.28425834],\n",
       "        [ 0.168024  ,  2.48913716,  0.1752861 , -1.92445701,  0.80018764],\n",
       "        [-0.60558536,  1.14028646, -1.10895183, -0.08452363,  0.85158695]]),\n",
       " array([[-1.60618542, -0.52428226,  1.24044823],\n",
       "        [-1.57434081, -1.75819934,  0.28291168],\n",
       "        [ 0.56223123, -0.10857519, -0.34100182],\n",
       "        [-1.23673802,  1.40066498,  0.60128136],\n",
       "        [ 0.31215675,  0.52067189,  1.01235583],\n",
       "        [ 0.5332675 ,  1.26836963,  0.45983981]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_weights(n_inputs, n_hidden, n_output):\n",
    "    # Initialize weights from input to hidden layer\n",
    "    # We add +1 for the bias unit\n",
    "    W0 = np.random.randn(n_inputs + 1, n_hidden)\n",
    "    \n",
    "    # Initialize weights from hidden to hidden layer\n",
    "    W1 = np.random.randn(n_hidden + 1, n_hidden)\n",
    "    \n",
    "    # Initialize weights from hidden to output layer\n",
    "    W2 = np.random.randn(n_hidden + 1, n_output)\n",
    "    \n",
    "    return W0, W1, W2\n",
    "\n",
    "# Test the function\n",
    "W0, W1, W2 = init_weights(10, 5, 3)\n",
    "W0, W1, W2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d73440f",
   "metadata": {},
   "source": [
    "# LETS US TRY TO CREATE A SIGMOID FUNCTION AND FORWARD PROPAGATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc369d4f",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "### `sigmoid` Function:\n",
    "\n",
    "This function calculates the sigmoid activation, which is a common activation function in neural networks. The sigmoid function maps any input into a value between 0 and 1, making it useful for binary classification problems.\n",
    "\n",
    "#### Formula:\n",
    "\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]\n",
    "\n",
    "---\n",
    "\n",
    "### `feedforward` Function:\n",
    "\n",
    "This function represents the feedforward process of a neural network. During feedforward, the data moves from the input layer, through the hidden layers, and finally to the output layer.\n",
    "\n",
    "#### Parameters:\n",
    "- **`x`**: Input data.\n",
    "- **`W0`, `W1`, `W2`**: Weight matrices for the layers.\n",
    "\n",
    "#### Process:\n",
    "\n",
    "1. **Input Layer**:\n",
    "    - The activations `a0` are set to the input `x`.\n",
    "\n",
    "2. **Input to First Hidden Layer**:\n",
    "    - Pre-activations `z1` are calculated by multiplying the activations `a0` with the weight matrix `W0`.\n",
    "    - Activations `a1` are computed by applying the sigmoid function to `z1`.\n",
    "    - A bias unit (a column of ones) is added to the activations `a1`.\n",
    "\n",
    "3. **First Hidden Layer to Second Hidden Layer**:\n",
    "    - Pre-activations `z2` are calculated by multiplying `a1` with the weight matrix `W1`.\n",
    "    - Activations `a2` are computed by applying the sigmoid function to `z2`.\n",
    "    - A bias unit (a column of ones) is added to the activations `a2`.\n",
    "\n",
    "4. **Second Hidden Layer to Output Layer**:\n",
    "    - Pre-activations `z3` are calculated by multiplying `a2` with the weight matrix `W2`.\n",
    "    - Activations `a3` are computed by applying the sigmoid function to `z3`.\n",
    "\n",
    "#### Returns:\n",
    "The function returns the pre-activations `z1`, `z2`, `z3` and activations `a0`, `a1`, `a2`, `a3` for each layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "The function is then tested using a sample input `x_sample` of shape `(1, 11)`. The activations of the output layer `a3` are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9195446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1530338 , 0.7409219 , 0.89245982]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def feedforward(x, W0, W1, W2):\n",
    "    # Input layer activations\n",
    "    a0 = x\n",
    "    \n",
    "    # Input to Hidden layer\n",
    "    z1 = np.dot(a0, W0) #a.W\n",
    "    a1 = sigmoid(z1) #simoid(a.w)=sigmoid(Z)------implementation of function\n",
    "    \n",
    "    # Add bias to the activations of the hidden layer\n",
    "    a1 = np.concatenate((a1, np.ones((a1.shape[0], 1))), axis=1)\n",
    "    \n",
    "    # Hidden to Hidden layer\n",
    "    z2 = np.dot(a1, W1)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # Add bias to the activations of the hidden layer\n",
    "    a2 = np.concatenate((a2, np.ones((a2.shape[0], 1))), axis=1)\n",
    "    \n",
    "    # Hidden to Output layer\n",
    "    z3 = np.dot(a2, W2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return z1, z2, z3, a0, a1, a2, a3\n",
    "\n",
    "# Test the function with a sample input\n",
    "x_sample = np.random.randn(1, 11)\n",
    "z1, z2, z3, a0, a1, a2, a3 = feedforward(x_sample, W0, W1, W2)\n",
    "a3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f3f3f7",
   "metadata": {},
   "source": [
    "# LET US TRY TO IMPLEMENT PREDICT FUNCTION AND TEST IT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a4039",
   "metadata": {},
   "source": [
    "# `predict` Function Explanation:\n",
    "\n",
    "This function is designed to predict the output of the neural network given a set of input data.\n",
    "\n",
    "### Parameters:\n",
    "- **`x`**: The input data for which we want to make a prediction.\n",
    "- **`W0`, `W1`, `W2`**: The weight matrices of the neural network.\n",
    "\n",
    "### Inside the function:\n",
    "\n",
    "1. **Feedforward Process**:\n",
    "    - The function utilizes the previously defined `feedforward` function to get the activations of the output layer, `a3`.\n",
    "    - While the `feedforward` function returns the activations and pre-activations for all layers, the `predict` function only requires the final output, so we use `_` to discard the other returned values.\n",
    "\n",
    "2. **Return**: \n",
    "    - The function then returns the activations of the output layer, `a3`, as the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "After defining the `predict` function, we test it using a sample input, `x_test`, with a shape of `(1, 11)`. The predicted output, `y_pred`, is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95f1398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26239176, 0.33761267, 0.33455973]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def predict(x, W0, W1, W2):\n",
    "    _, _, _, _, _, _, a3 = feedforward(x, W0, W1, W2)\n",
    "    return a3\n",
    "\n",
    "# Test the function with a sample input\n",
    "x_test = np.random.randn(1, 11)\n",
    "y_pred = predict(x_test, W0, W1, W2)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f5207",
   "metadata": {},
   "source": [
    "## `loss` Function Explanation:\n",
    "\n",
    "The `loss` function computes the Mean Squared Error (MSE) loss, which measures the average squared differences between the predicted and actual values.\n",
    "\n",
    "### Formula for Mean Squared Error:\n",
    "\\[ \\text{MSE} = \\frac{1}{2m} \\sum_{i=1}^{m} (Y_{\\text{pred}[i]} - Y_{\\text{actual}[i]})^2 \\]\n",
    "Where:\n",
    "- \\( m \\) is the number of examples.\n",
    "- \\( Y_{\\text{pred}} \\) is the predicted output.\n",
    "- \\( Y_{\\text{actual}} \\) is the actual output.\n",
    "\n",
    "### Inside the function:\n",
    "\n",
    "1. **Number of Examples**:\n",
    "    - The variable `m` represents the number of examples in our dataset, which is derived from the shape of the actual values, `Y`.\n",
    "\n",
    "2. **Loss Computation**:\n",
    "    - The difference between the predicted values (`Y_pred`) and the actual values (`Y`) is squared.\n",
    "    - The squared differences are summed up.\n",
    "    - The sum is then divided by `2m` to get the average loss.\n",
    "\n",
    "3. **Return**: \n",
    "    - The function returns the computed MSE loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "After defining the `loss` function, we test its accuracy using:\n",
    "- A sample predicted output, `y_pred`.\n",
    "- A sample actual output, `Y_actual` with the shape `(1, 3)`.\n",
    "\n",
    "The computed loss value, `loss_value`, is then printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf25fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3097683100963687"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(Y_pred, Y):\n",
    "    \"\"\"Mean Squared Error Loss\"\"\"\n",
    "    m = Y.shape[0]  # number of examples\n",
    "    loss = (1 / (2 * m)) * np.sum((Y_pred - Y) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Test the function with a sample prediction and actual value\n",
    "Y_actual = np.array([[0, 1, 0]])\n",
    "loss_value = loss(y_pred, Y_actual)\n",
    "loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07c6ae",
   "metadata": {},
   "source": [
    "## `backprop` Function Explanation:\n",
    "\n",
    "The `backprop` function performs the backpropagation algorithm, which is an essential component in training a neural network. Backpropagation adjusts the weights of the network to reduce the prediction error.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`X_train`**: The input training data.\n",
    "- **`Y_train`**: The actual output for the training data.\n",
    "- **`W0`, `W1`, `W2`**: The weight matrices of the neural network.\n",
    "- **`learning_rate`**: The rate at which the weights are updated.\n",
    "\n",
    "### Inside the function:\n",
    "\n",
    "1. **Forward Pass**:\n",
    "    - The feedforward process is executed to obtain the pre-activations (`z1`, `z2`, `z3`) and activations (`a0`, `a1`, `a2`, `a3`) for each layer.\n",
    "\n",
    "2. **Loss Derivative with Respect to Output**:\n",
    "    - The derivative of the loss with respect to the output, `dZ3`, is computed. It represents the difference between the predicted output (`a3`) and the actual values (`Y_train`).\n",
    "    - The gradient of the weights connecting the second hidden layer to the output layer, `dW2`, is computed using `dZ3` and the activations `a2`.\n",
    "\n",
    "3. **Backpropagate Through Second Hidden Layer**:\n",
    "    - The error is backpropagated to the second hidden layer using the transpose of the weight matrix `W2` and the derivative of the sigmoid activation function.\n",
    "    - The bias term is removed from the error term `dZ2`.\n",
    "    - The gradient of the weights connecting the first and second hidden layers, `dW1`, is computed using `dZ2` and the activations `a1`.\n",
    "\n",
    "4. **Backpropagate Through First Hidden Layer**:\n",
    "    - The error is further backpropagated to the first hidden layer using the transpose of the weight matrix `W1` and the derivative of the sigmoid activation function.\n",
    "    - The bias term is removed from the error term `dZ1`.\n",
    "    - The gradient of the weights connecting the input layer to the first hidden layer, `dW0`, is computed using `dZ1` and the input data `a0`.\n",
    "\n",
    "5. **Update Weights**:\n",
    "    - The weights `W0`, `W1`, and `W2` are updated by subtracting the product of the learning rate and their respective gradients.\n",
    "\n",
    "### Returns:\n",
    "The function returns the updated weights: `W0`, `W1`, and `W2`.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "After defining the `backprop` function, we test it using:\n",
    "- A sample input, `x_test`.\n",
    "- A sample actual output, `Y_actual`.\n",
    "- The weight matrices `W0`, `W1`, and `W2`.\n",
    "- A learning rate of 0.1.\n",
    "\n",
    "The updated weights `W0`, `W1`, and `W2` are then\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67051463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.29993992,  0.13852882, -1.07631499, -0.68175352, -1.151879  ],\n",
       "        [ 0.09221523,  0.61198514, -0.97299952,  1.12463525,  0.533792  ],\n",
       "        [ 0.22216159,  0.02154447,  1.6690609 , -0.6381654 , -1.0477523 ],\n",
       "        [-0.29761462, -0.45083646,  0.04203293, -0.50543978, -1.25670446],\n",
       "        [ 0.42881818, -0.37518099,  1.1830802 , -1.19715452, -0.95154033],\n",
       "        [ 1.87365463,  1.15422887,  0.45363987,  1.06798323,  0.15294225],\n",
       "        [-0.55625747, -0.98265547, -0.64691473, -0.52185492,  0.68266024],\n",
       "        [-1.68681482, -0.04727326,  1.27319868, -0.6526455 , -1.23959163],\n",
       "        [ 1.17078997,  0.02678696,  2.26900094, -0.63290529,  0.23028465],\n",
       "        [ 0.18311615,  0.11315367, -0.67584535,  0.02047485,  0.51947268],\n",
       "        [ 0.611685  , -1.26801703,  0.66129763, -1.50272506,  0.39432722]]),\n",
       " array([[-1.20445494,  2.14900468,  1.14122057,  1.38083286,  0.94568505],\n",
       "        [ 0.38835061, -0.14254374, -0.7378739 ,  3.43372055, -0.04512435],\n",
       "        [-0.96948964,  0.3720483 ,  0.20853184,  0.8192532 , -0.81797517],\n",
       "        [-1.22585956, -0.74410319,  0.01395964,  0.41450313, -0.29461864],\n",
       "        [ 0.16491037,  2.48892306,  0.17966625, -1.9245443 ,  0.78884627],\n",
       "        [-0.60904633,  1.14004848, -1.10408305, -0.08462065,  0.8389804 ]]),\n",
       " array([[-1.60669997, -0.52347051,  1.23722976],\n",
       "        [-1.58842015, -1.73598733,  0.19484527],\n",
       "        [ 0.55608   , -0.09887082, -0.37947779],\n",
       "        [-1.25058249,  1.42250645,  0.51468411],\n",
       "        [ 0.30066423,  0.53880285,  0.94047006],\n",
       "        [ 0.51910754,  1.29070883,  0.37126917]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def backprop(X_train, Y_train, W0, W1, W2, learning_rate):\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    # Forward pass\n",
    "    z1, z2, z3, a0, a1, a2, a3 = feedforward(X_train, W0, W1, W2)\n",
    "    \n",
    "    # Calculate the loss derivative w.r.t the output\n",
    "    dZ3 = a3 - Y_train\n",
    "    dW2 = (1 / m) * np.dot(a2.T, dZ3)\n",
    "    \n",
    "    # Backpropagate through the second hidden layer\n",
    "    dZ2 = np.dot(dZ3, W2.T) * (a2 * (1 - a2))\n",
    "    dZ2 = dZ2[:, :-1]  # Remove the bias term\n",
    "    dW1 = (1 / m) * np.dot(a1.T, dZ2)\n",
    "    \n",
    "    # Backpropagate through the first hidden layer\n",
    "    dZ1 = np.dot(dZ2, W1.T) * (a1 * (1 - a1))\n",
    "    dZ1 = dZ1[:, :-1]  # Remove the bias term\n",
    "    dW0 = (1 / m) * np.dot(a0.T, dZ1)\n",
    "    \n",
    "    # Update weights\n",
    "    W0 -= learning_rate * dW0\n",
    "    W1 -= learning_rate * dW1\n",
    "    W2 -= learning_rate * dW2\n",
    "    \n",
    "    return W0, W1, W2\n",
    "\n",
    "# Test the function with sample data\n",
    "W0, W1, W2 = backprop(x_test, Y_actual, W0, W1, W2, learning_rate=0.1)\n",
    "W0, W1, W2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edb555",
   "metadata": {},
   "source": [
    "## `train` Function Explanation:\n",
    "\n",
    "The `train` function trains the neural network using the provided training data and labels.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`X_train`**: The input training data.\n",
    "- **`Y_train`**: The actual output for the training data.\n",
    "- **`n_inputs`**: Number of input nodes.\n",
    "- **`n_hidden`**: Number of nodes in each of the two hidden layers.\n",
    "- **`n_output`**: Number of output nodes.\n",
    "- **`n_epochs`**: Number of training iterations.\n",
    "- **`learning_rate`**: The rate at which the weights are updated.\n",
    "\n",
    "### Inside the function:\n",
    "\n",
    "1. **Weight Initialization**:\n",
    "    - The weights for the network are initialized using the `init_weights` function.\n",
    "\n",
    "2. **Training Loop**:\n",
    "    - For each epoch, the following steps are performed:\n",
    "        - The `backprop` function is called to adjust the weights using the backpropagation algorithm.\n",
    "        - Every 10 epochs, the current loss is computed using the `predict` and `loss` functions and is then printed to provide insight into the training process.\n",
    "\n",
    "3. **Return**: \n",
    "    - The function returns the trained weights: `W0`, `W1`, and `W2`.\n",
    "\n",
    "---\n",
    "\n",
    "### Test:\n",
    "\n",
    "The `train` function is tested using the following steps:\n",
    "\n",
    "1. **Sample Data Generation**:\n",
    "    - A training dataset `X_train` of shape `(1000, 10)` is randomly generated.\n",
    "    - A bias column `b` is added to the `X_train` dataset.\n",
    "\n",
    "2. **Class Labels Generation**:\n",
    "    - Random class labels are generated for `Y_train`. Each row corresponds to a one-hot encoded class label, meaning only one element in each row is set to 1, while the others are set to 0.\n",
    "\n",
    "3. **Training the Network**:\n",
    "    - The neural network is trained using the `train` function with the generated sample data, with 100 epochs and a learning rate of 0.1.\n",
    "    - The trained weights `W0`, `W1`, and `W2` are returned.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e600e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.4708\n",
      "Epoch 10, Loss: 0.3979\n",
      "Epoch 20, Loss: 0.3596\n",
      "Epoch 30, Loss: 0.3447\n",
      "Epoch 40, Loss: 0.3392\n",
      "Epoch 50, Loss: 0.3370\n",
      "Epoch 60, Loss: 0.3361\n",
      "Epoch 70, Loss: 0.3356\n",
      "Epoch 80, Loss: 0.3353\n",
      "Epoch 90, Loss: 0.3352\n"
     ]
    }
   ],
   "source": [
    "def train(X_train, Y_train, n_inputs, n_hidden, n_output, n_epochs, learning_rate):\n",
    "    # Initialize weights\n",
    "    W0, W1, W2 = init_weights(n_inputs, n_hidden, n_output)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        # Feedforward and Backpropagation\n",
    "        W0, W1, W2 = backprop(X_train, Y_train, W0, W1, W2, learning_rate)\n",
    "        \n",
    "        # Compute loss for logging\n",
    "        if epoch % 10 == 0:\n",
    "            y_pred = predict(X_train, W0, W1, W2)\n",
    "            current_loss = loss(y_pred, Y_train)\n",
    "            print(f\"Epoch {epoch}, Loss: {current_loss:.4f}\")\n",
    "            \n",
    "    return W0, W1, W2\n",
    "\n",
    "# Test the training function using the sample data provided in the problem\n",
    "n_samples = 1000\n",
    "n_inputs = 10\n",
    "n_hidden = 5\n",
    "n_output = 3\n",
    "X_train = np.random.randn(n_samples, 10)\n",
    "b = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((X_train, b), axis=1)\n",
    "\n",
    "# Generate random class labels for Y_train (0 or 1)\n",
    "Y_train = np.zeros((n_samples, n_output), dtype=int)\n",
    "for i in range(n_samples):\n",
    "    random_idx = np.random.randint(n_output)\n",
    "    Y_train[i, random_idx] = 1\n",
    "\n",
    "# Train the network\n",
    "W0, W1, W2 = train(X_train, Y_train, n_inputs, n_hidden, n_output, n_epochs=100, learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b304561",
   "metadata": {},
   "source": [
    "# LETS TEST AND PREDICT IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07fd5681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30547235, 0.36754361, 0.26902723]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the network with a sample input\n",
    "x_test = np.random.randn(1, 11)\n",
    "y_pred = predict(x_test, W0, W1, W2)\n",
    "y_pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
